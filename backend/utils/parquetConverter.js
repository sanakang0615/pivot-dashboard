const fs = require('fs');
const path = require('path');
const { spawn } = require('child_process');

class ParquetConverter {
  constructor(dataDirectory) {
    this.dataDirectory = dataDirectory;
    this.convertedDirectory = path.join(dataDirectory, 'converted');
    this.ensureConvertedDirectory();
  }

  ensureConvertedDirectory() {
    if (!fs.existsSync(this.convertedDirectory)) {
      fs.mkdirSync(this.convertedDirectory, { recursive: true });
      console.log(`ğŸ“ Created converted directory: ${this.convertedDirectory}`);
    }
  }

  /**
   * Parquet íŒŒì¼ì„ CSVë¡œ ë³€í™˜
   * @param {string} datasetId - ë°ì´í„°ì…‹ ID
   * @returns {Promise<string>} - ë³€í™˜ëœ CSV íŒŒì¼ ê²½ë¡œ
   */
  async convertParquetToCSV(datasetId) {
    const parquetPath = path.join(this.dataDirectory, `${datasetId}.parquet`);
    const csvPath = path.join(this.convertedDirectory, `${datasetId}.csv`);
    
    console.log(`ğŸ”„ Converting parquet to CSV: ${datasetId}`);
    console.log(`ğŸ“‚ Source: ${parquetPath}`);
    console.log(`ğŸ“‚ Target: ${csvPath}`);

    // 1. Parquet íŒŒì¼ ì¡´ì¬ í™•ì¸
    if (!fs.existsSync(parquetPath)) {
      throw new Error(`Parquet file not found: ${parquetPath}`);
    }

    // 2. ì´ë¯¸ ë³€í™˜ëœ CSVê°€ ìˆê³  ìµœì‹ ì¸ì§€ í™•ì¸
    if (await this.isCsvUpToDate(parquetPath, csvPath)) {
      console.log(`âœ… Using existing CSV file: ${csvPath}`);
      return csvPath;
    }

    // 3. Pythonì„ ì‚¬ìš©í•œ ë³€í™˜ ì‹œë„
    try {
      await this.convertUsingPython(parquetPath, csvPath);
      console.log(`âœ… Converted using Python: ${csvPath}`);
      return csvPath;
    } catch (pythonError) {
      console.warn(`âš ï¸ Python conversion failed: ${pythonError.message}`);
    }

    // 4. ëŒ€ì•ˆ: ë¯¸ë¦¬ ì¤€ë¹„ëœ CSV íŒŒì¼ ì‚¬ìš©
    const fallbackCsvPath = path.join(this.dataDirectory, `${datasetId}_fallback.csv`);
    if (fs.existsSync(fallbackCsvPath)) {
      console.log(`ğŸ“‹ Using fallback CSV: ${fallbackCsvPath}`);
      // ë¯¸ë¦¬ ì¤€ë¹„ëœ CSVë¥¼ converted ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
      fs.copyFileSync(fallbackCsvPath, csvPath);
      return csvPath;
    }

    throw new Error(`Failed to convert parquet file and no fallback available: ${datasetId}`);
  }

  /**
   * Pythonì„ ì‚¬ìš©í•˜ì—¬ Parquetì„ CSVë¡œ ë³€í™˜
   */
  async convertUsingPython(parquetPath, csvPath) {
    return new Promise((resolve, reject) => {
      const pythonScript = this.generatePythonScript();
      const scriptPath = path.join(this.convertedDirectory, 'convert_parquet.py');
      
      // Python ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìƒì„±
      fs.writeFileSync(scriptPath, pythonScript);

      // Python ì‹¤í–‰ (ê°€ìƒí™˜ê²½ ì‚¬ìš©)
      const pythonPath = path.join(__dirname, '..', 'parquet_env', 'bin', 'python3');
      const python = spawn(pythonPath, [scriptPath, parquetPath, csvPath], {
        stdio: ['pipe', 'pipe', 'pipe']
      });

      let stdout = '';
      let stderr = '';

      python.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      python.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      python.on('close', (code) => {
        // ì„ì‹œ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì‚­ì œ
        try {
          fs.unlinkSync(scriptPath);
        } catch (e) {
          console.warn('Failed to cleanup Python script:', e.message);
        }

        if (code === 0 && fs.existsSync(csvPath)) {
          console.log(`ğŸ Python conversion successful: ${stdout.trim()}`);
          resolve(csvPath);
        } else {
          console.error(`ğŸ Python conversion failed (code ${code}):`, stderr);
          reject(new Error(`Python conversion failed: ${stderr || 'Unknown error'}`));
        }
      });

      python.on('error', (error) => {
        console.error(`ğŸ Python process error:`, error);
        reject(new Error(`Failed to start Python process: ${error.message}`));
      });
    });
  }

  /**
   * Python ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
   */
  generatePythonScript() {
    return `#!/usr/bin/env python3
import pandas as pd
import sys
import os

def convert_parquet_to_csv(parquet_file, csv_file):
    try:
        print(f"Reading parquet file: {parquet_file}")
        
        # Parquet íŒŒì¼ ì½ê¸°
        df = pd.read_parquet(parquet_file)
        
        print(f"Data shape: {df.shape}")
        print(f"Columns: {list(df.columns)}")
        
        # CSVë¡œ ì €ì¥
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        print(f"Successfully converted to CSV: {csv_file}")
        print(f"Output file size: {os.path.getsize(csv_file)} bytes")
        
    except Exception as e:
        print(f"Error during conversion: {str(e)}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python convert_parquet.py <input.parquet> <output.csv>", file=sys.stderr)
        sys.exit(1)
    
    parquet_file = sys.argv[1]
    csv_file = sys.argv[2]
    
    convert_parquet_to_csv(parquet_file, csv_file)
`;
  }

  /**
   * CSV íŒŒì¼ì´ ìµœì‹ ì¸ì§€ í™•ì¸
   */
  async isCsvUpToDate(parquetPath, csvPath) {
    try {
      if (!fs.existsSync(csvPath)) {
        return false;
      }

      const parquetStats = fs.statSync(parquetPath);
      const csvStats = fs.statSync(csvPath);

      // CSVê°€ Parquetë³´ë‹¤ ìµœê·¼ì— ìƒì„±ë˜ì—ˆìœ¼ë©´ ìµœì‹ ìœ¼ë¡œ íŒë‹¨
      return csvStats.mtime >= parquetStats.mtime;
    } catch (error) {
      return false;
    }
  }

  /**
   * ë°ì´í„°ì…‹ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
   */
  getAvailableDatasets() {
    const datasets = [];
    
    try {
      const files = fs.readdirSync(this.dataDirectory);
      
      files.forEach(file => {
        if (file.endsWith('.parquet')) {
          const datasetId = file.replace('.parquet', '');
          const parquetPath = path.join(this.dataDirectory, file);
          const csvPath = path.join(this.convertedDirectory, `${datasetId}.csv`);
          
          const stats = fs.statSync(parquetPath);
          const hasConvertedCsv = fs.existsSync(csvPath);
          
          datasets.push({
            id: datasetId,
            name: this.getDatasetName(datasetId),
            parquetFile: file,
            parquetSize: stats.size,
            hasConvertedCsv,
            lastModified: stats.mtime
          });
        }
      });
    } catch (error) {
      console.error('Error reading data directory:', error);
    }
    
    return datasets;
  }

  /**
   * ë°ì´í„°ì…‹ ì´ë¦„ ë§¤í•‘
   */
  getDatasetName(datasetId) {
    const nameMap = {
      'campaign_data': 'Campaign Data',
      'adpack_data': 'AdPack Data'
    };
    return nameMap[datasetId] || datasetId;
  }

  /**
   * ë³€í™˜ëœ ëª¨ë“  CSV íŒŒì¼ ì •ë¦¬
   */
  cleanupConvertedFiles() {
    try {
      const files = fs.readdirSync(this.convertedDirectory);
      files.forEach(file => {
        const filePath = path.join(this.convertedDirectory, file);
        fs.unlinkSync(filePath);
      });
      console.log(`ğŸ—‘ï¸ Cleaned up ${files.length} converted files`);
    } catch (error) {
      console.error('Error during cleanup:', error);
    }
  }
}

module.exports = ParquetConverter; 